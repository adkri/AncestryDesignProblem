{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMd70+7ak4BBLY6iSqw+BHx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sparlly007/AncestryDesignProblem/blob/master/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install scikit-learn\n",
        "!pip install scikit-learn\n",
        "\n",
        "from typing import Callable, List, Tuple\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "import collections\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "metadata": {
        "id": "wjtRTCYSylwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = datasets.load_metric(\"precision\")\n",
        "dataSet = load_dataset('imdb', 'plain_text', split='train')\n",
        "testDataSet = load_dataset('imdb', 'plain_text', split='test')\n",
        "\n",
        "smsDataSet = load_dataset('sms_spam', 'plain_text', split='train')\n",
        "smsDataSet[:15]['sms'][13]"
      ],
      "metadata": {
        "id": "gLn-mgyl9d6z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "9cafd56e-a726-451e-d2c2-0b7b0ad8fd40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
            "Reusing dataset sms_spam (/root/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet[:3]['text'][2]\n",
        "#reference_batch = smsDataSet[:3]['sms']\n",
        "#sys_batch = smsDataSet[:15]['sms'][13]\n",
        "#precision.add_batch(predictions=sys_batch, references=reference_batch)\n",
        "#score = precision.compute()\n",
        "#score\n",
        "#dataSet.features['label']"
      ],
      "metadata": {
        "id": "lwxIIQ6w68Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(cleaned : List[str]):\n",
        "  # create a set containing all the tokens that are only letters\n",
        "  # the set will automatically filter out duplicates\n",
        "  words = {w.lower() for w in cleaned if w.isalpha()}\n",
        "  # convert the set to a list before returning.\n",
        "  return list(words)"
      ],
      "metadata": {
        "id": "u47QvTQuzLXE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(cleaned : List[str]):\n",
        "  # create a set containing all the tokens that are only letters\n",
        "  # the set will automatically filter out duplicates\n",
        "  words = {w.lower() for w in cleaned if w.isalpha()}\n",
        "  # convert the set to a list before returning.\n",
        "  return list(words)"
      ],
      "metadata": {
        "id": "QTfQF9dQ2_1f"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text: str):\n",
        "  test = \"\"\n",
        "  for i in text:\n",
        "    test = test+i\n",
        "  test = test.lower()\n",
        "  #Removes punctuation\n",
        "  res = re.sub(r'[^\\w\\s]', '', test)\n",
        "  #Removes numbers\n",
        "  res = ''.join([i for i in res if not i.isdigit()])\n",
        "  #Tokenizes string by space\n",
        "  #res = word_tokenize(res)\n",
        "  #Removes one-char words\n",
        "  #res = [i for i in res if len(i) > 1]\n",
        "  return res\n",
        "\n",
        "#clean(dataSet[:20]['text'])"
      ],
      "metadata": {
        "id": "3lrmirq-_SXa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learn(data: List[Tuple[str, int]]):\n",
        "  vocab = [] #Our list of vocabs included in the dataset\n",
        "  docs = [] #Will contain the list of reviews of the dataset after cleaning them\n",
        "  grouped = {} #Contains classes with their documents combined\n",
        "  classFreq = {} #Displays frequency of words of each class\n",
        "  probTable = {} #Will contain the probability of each feature in context of its class\n",
        "  probClass = {} #Probability of each class among total documents\n",
        "  features = {} #Will contain features to be extracted\n",
        "\n",
        "  #Combines document of the same classes to their respective classes through dictionary\n",
        "  for i, j in data:\n",
        "    docs.append(i)\n",
        "    i = clean(i)\n",
        "    if j not in grouped.keys():\n",
        "      grouped[j] = i\n",
        "      probClass[j] = 1\n",
        "    else:\n",
        "      grouped[j] += i\n",
        "      probClass[j] += 1\n",
        "\n",
        "  #Creates vocab list  \n",
        "  for i, j in data:\n",
        "    for k in word_tokenize(i):\n",
        "      if k not in vocab:\n",
        "        vocab.append(k)\n",
        "\n",
        "  #Extracts features with the bag-of-words feature extractor\n",
        "  for i in grouped.keys():\n",
        "    features[i] = bag_of_words(grouped[i])\n",
        "    \n",
        "  #Gets probability for every class to appear among all docs\n",
        "  for i in probClass.keys():\n",
        "    probClass[i] = probClass[i] / len(docs)\n",
        "\n",
        "  #Attaches a counter to each dictionary entry\n",
        "  for i, j in data:\n",
        "    classFreq[j] = collections.Counter(grouped[j])\n",
        "\n",
        "  #Attaches a dictionary of feature probabilities to each class\n",
        "  for i in classFreq.keys():\n",
        "    probTable[i] = {}\n",
        "    for j in classFreq[i]:\n",
        "      if j in features[i]:\n",
        "        prob = (classFreq[i][j] + 1) / (len(classFreq[i]) + 1)\n",
        "        probTable[i][j] = prob\n",
        "\n",
        "  return probTable, probClass, vocab\n"
      ],
      "metadata": {
        "id": "c1VKyOQRgqyw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(document: str, model, classProb, vocab):\n",
        "  sum = {}\n",
        "  document = clean(document)\n",
        "  for i in classProb.keys():\n",
        "    sum[i] = classProb[i]\n",
        "  \n",
        "  for i in model.keys():\n",
        "    for j in document:\n",
        "      if j in vocab and j in model[i].keys():\n",
        "        sum[i] = sum[i] * model[i][j]\n",
        "\n",
        "  return max(sum, key=sum.get)\n"
      ],
      "metadata": {
        "id": "kTb-AehdTnDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab = [] #Our list of vocabs included in the dataset\n",
        "docs = [] #Will contain the list of reviews of the dataset after cleaning them\n",
        "#text = dataSet[:3]['text'] #List of all reviews in dataset\n",
        "#classes = dataSet[:3]['label'] #List of all classes in dataset to each review respective of index\n",
        "text = smsDataSet[:10]['sms'] #List of all reviews in dataset\n",
        "classes = smsDataSet[:10]['label'] #List of all classes in dataset to each review respective of index\n",
        "grouped = {}\n",
        "classFreq = {} #Displays frequency of words of each class\n",
        "probTable = {}\n",
        "\n",
        "#Concatenates each review to a list\n",
        "for i in range(len(text)):\n",
        "   docs.append(text[i])\n",
        "\n",
        "rest = word_tokenize(dataSet[:3]['text'][0])\n",
        "#for x in tokenized: \n",
        "#  if x not in vocab:\n",
        "#    vocab.append(x)\n",
        "\n",
        "#Classifies each string with its class\n",
        "classified = []\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  classified.append((docs[i], classes[i]))\n",
        "\n",
        "result = learn(classified)\n",
        "result\n",
        "classify(smsDataSet[:15]['sms'][10], result[0], result[1], result[2])\n",
        "#bag_of_words(rest)\n",
        "\n",
        "#grouped = {}\n",
        "#for i, j in classified:\n",
        " #   i = clean(i)\n",
        " #   if j not in grouped.keys():\n",
        " #     grouped[j] = i\n",
        "#    else:\n",
        " #     grouped[j] += i\n",
        "\n",
        "#bag_of_words(grouped[0])\n",
        "#grouped[1]"
      ],
      "metadata": {
        "id": "kVNL9IM6iYR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12090129-be95-4b4f-84dd-f703ae6b4df1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}